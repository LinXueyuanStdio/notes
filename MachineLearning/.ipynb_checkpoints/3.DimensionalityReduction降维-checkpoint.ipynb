{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 奇异值与特征值基础知识："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "特征值分解和奇异值分解在机器学习领域都是属于满地可见的方法。两者有着很紧密的关系，我在接下来会谈到，特征值分解和奇异值分解的目的都是一样，就是提取出一个矩阵最重要的特征。先谈谈特征值分解吧："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 1）特征值：\n",
    "\n",
    "如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：\n",
    "\n",
    "$$Av=\\lambda v$$\n",
    "\n",
    "这时候λ就被称为特征向量v对应的特征值，一个矩阵的一组特征向量是一组正交向量。特征值分解是将一个矩阵分解成下面的形式：\n",
    "\n",
    "$$A=Q\\sum Q$$\n",
    "\n",
    "其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角阵，每一个对角线上的元素就是一个特征值。我这里引用了一些参考文献中的内容来说明一下。首先，要明确的是，一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。比如说下面的一个矩阵：\n",
    "\n",
    "$$M=\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "    \n",
    "它其实对应的线性变换是下面的形式：\n",
    "\n",
    "![](matrix0.png)\n",
    "\n",
    "因为这个矩阵M乘以一个向量(x,y)的结果是：\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3x\\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "上面的矩阵是对称的，所以这个变换是一个对x，y轴的方向一个拉伸变换\n",
    "（每一个对角线上的元素将会对一个维度进行拉伸变换，\n",
    "当值>1时，是拉长，\n",
    "当值<1时时缩短）\n",
    "\n",
    "当矩阵不是对称的时候，假如说矩阵是下面的样子：\n",
    "\n",
    "$$M=\n",
    "\\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "它所描述的变换是下面的样子：\n",
    "\n",
    "![](matrix1.png)\n",
    "\n",
    "这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最主要的变化方向（变化方向可能有不止一个），如果我们想要描述好一个变换，那我们就描述好这个变换主要的变化方向就好了。反过头来看看之前特征值分解的式子，分解得到的Σ矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）\n",
    "\n",
    "当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。也就是之前说的：提取这个矩阵最重要的特征。\n",
    "\n",
    "总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。\n",
    "\n",
    "（说了这么多特征值变换，不知道有没有说清楚，请各位多提提意见。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 2）奇异值：\n",
    "\n",
    "下面谈谈奇异值分解。特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法：\n",
    "    \n",
    "$$A=U\\sum V^T$$\n",
    "\n",
    "假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量），从图片来反映几个相乘的矩阵的大小可得下面的图片\n",
    "\n",
    "![](matrix2.png)\n",
    "\n",
    "那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置 * A，将会得到一个方阵，我们用这个方阵求特征值可以得到：\n",
    "\n",
    "$$ (A^T A)v_i=\\lambda_i v_i $$\n",
    "\n",
    "这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：\n",
    "\n",
    "$$ \\sigma_i=\\sqrt{\\lambda_i} $$\n",
    "$$ u_i = \\frac{1}{\\sigma_i}Av_i $$\n",
    "\n",
    "这里的$\\sigma$就是上面说的奇异值，$u$就是上面说的左奇异向量。奇异值$\\sigma$跟特征值类似，在矩阵$\\sum$中也是从大到小排列，而且$\\sigma$的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前$r$大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：\n",
    "\n",
    "$$ A_{m\\times n} \\approx U_{m\\times r}\\Sigma_{r\\times r} V^T_{r\\times n}$$\n",
    "\n",
    "r是一个远小于m、n的数，这样矩阵的乘法看起来像是下面的样子：\n",
    "\n",
    "![](PCA0.png)\n",
    "\n",
    "右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵：$U、\\Sigma、V$就好了。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 奇异值的计算："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "奇异值的计算是一个难题，是一个$O(N^3)$的算法。在单机的情况下当然是没问题的，matlab在一秒钟内就可以算出1000 * 1000的矩阵的所有奇异值，但是当矩阵的规模增长的时候，计算的复杂度呈3次方增长，就需要并行计算参与了。Google的吴军老师在数学之美系列谈到SVD的时候，说起Google实现了SVD的并行化算法，说这是对人类的一个贡献，但是也没有给出具体的计算规模，也没有给出太多有价值的信息。\n",
    "\n",
    "其实SVD还是可以用并行的方式去实现的，在解大规模的矩阵的时候，一般使用迭代的方法，当矩阵的规模很大（比如说上亿）的时候，迭代的次数也可能会上亿次，如果使用Map-Reduce框架去解，则每次Map-Reduce完成的时候，都会涉及到写文件、读文件的操作。个人猜测Google云计算体系中除了Map-Reduce以外应该还有类似于MPI的计算模型，也就是节点之间是保持通信，数据是常驻在内存中的，这种计算模型比Map-Reduce在解决迭代次数非常多的时候，要快了很多倍。\n",
    "\n",
    "`Lanczos迭代`就是一种解对称方阵部分特征值的方法（之前谈到了，解$A’* A$得到的对称方阵的特征值就是解A的右奇异向量），是将一个对称的方程化为一个三对角矩阵再进行求解。\n",
    "\n",
    "按网上的一些文献来看，Google应该是用这种方法去做的奇异值分解的。\n",
    "\n",
    "请见Wikipedia上面的一些引用的论文，如果理解了那些论文，也“几乎”可以做出一个SVD了。\n",
    "\n",
    "由于奇异值的计算是一个很枯燥，纯数学的过程，而且前人的研究成果（论文中）几乎已经把整个程序的流程图给出来了。\n",
    "\n",
    "更多的关于奇异值计算的部分，将在后面的参考文献中给出，这里不再深入，我还是focus在奇异值的应用中去。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 奇异值与主成分分析（PCA）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "主成分分析在上一节里面也讲了一些，这里主要谈谈如何用SVD去解PCA的问题。PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子：\n",
    "\n",
    "![](PCA1.png)\n",
    "\n",
    "这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置.\n",
    "\n",
    "假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。\n",
    "\n",
    "如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。\n",
    "\n",
    "但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。\n",
    "\n",
    "一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。\n",
    "\n",
    "PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。\n",
    "\n",
    "还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。\n",
    "\n",
    "$$ A_{m\\times n}P_{n\\times n}=\\widetilde{A}_{m\\times n} $$\n",
    "\n",
    "而将一个$m \\times n$的矩阵A变换成一个$m \\times r$的矩阵，这样就会使得本来有n个feature的，变成了有r个feature了（r < n)，这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是：\n",
    "\n",
    "$$ A_{m\\times n}P_{n\\times r}=\\widetilde{A}_{m\\times r} $$ \n",
    "\n",
    "但是这个怎么和SVD扯上关系呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量…我们回忆一下之前得到的SVD式子：\n",
    "\n",
    "$$ A_{m\\times n}\\approx U_{m\\times r}\\Sigma_{r\\times r}V^T_{r\\times n} $$\n",
    "\n",
    "在矩阵的两边同时乘上一个矩阵V，由于V是一个正交的矩阵，所以V转置乘以V得到单位阵I，所以可以化成后面的式子\n",
    "\n",
    "$$ A_{m\\times n}V_{r\\times n}\\approx U_{m\\times r}\\Sigma_{r\\times r}V^T_{r\\times n}V_{r\\times n} $$\n",
    "$$ A_{m\\times n}V_{r\\times n}\\approx U_{m\\times r}\\Sigma_{r\\times r} $$\n",
    "\n",
    "将后面的式子与A * P那个m * n的矩阵变换为m * r的矩阵的式子对照看看，在这里，其实V就是P，也就是一个变化的向量。\n",
    "\n",
    "这里是将一个m * n 的矩阵压缩到一个m * r的矩阵，也就是对列进行压缩，如果我们想对行进行压缩（在PCA的观点下，对行进行压缩可以理解为，将一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉）怎么办呢？同样我们写出一个通用的行压缩例子：\n",
    "\n",
    "$$ P_{r\\times m}A_{m\\times n}=\\widetilde{A}_{r\\times n} $$\n",
    "\n",
    "这样就从一个m行的矩阵压缩到一个r行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以$U$的转置$U'$\n",
    "\n",
    "$$ U^T_{r\\times m}A_{m\\times n}\\approx \\Sigma_{r\\times r}V^T_{r\\times n} $$\n",
    "\n",
    "这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 奇异值与潜在语义索引LSI："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在矩阵计算与文本处理中的分类问题中谈到：\n",
    "\n",
    "- “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。”\n",
    "\n",
    "上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial，具体的网址我将在最后的引用中给出：\n",
    "\n",
    "![](PCA2.png)\n",
    "\n",
    "这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵：\n",
    "\n",
    "image      左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。\n",
    "\n",
    "![](PCA3.png)\n",
    "\n",
    "继续看这个矩阵还可以发现一些有意思的东西\n",
    "\n",
    "- 首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次；\n",
    "\n",
    "- 其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。\n",
    "\n",
    "- 然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到：\n",
    "\n",
    "![](PCA4.png)\n",
    "\n",
    "\n",
    "在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。\n",
    "    \n",
    "     \n",
    "[另一篇精彩的讲解](http://blog.csdn.net/redline2005/article/details/24100293)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "如果学习分类算法，最好从线性的入手，线性分类器最简单的就是LDA，它可以看做是简化版的SVM，如果想理解SVM这种分类器，那理解LDA就是很有必要的了。\n",
    "\n",
    "谈到LDA，就不得不谈谈PCA，PCA是一个和LDA非常相关的算法，从推导、求解、到算法最终的结果，都有着相当的相似。\n",
    "\n",
    "本次的内容主要是以推导数学公式为主，都是从算法的物理意义出发，然后一步一步最终推导到最终的式子，LDA和PCA最终的表现都是解一个矩阵特征值的问题，但是理解了如何推导，才能更深刻的理解其中的含义。\n",
    "\n",
    "本次内容要求读者有一些基本的线性代数基础，比如说特征值、特征向量的概念，空间投影，点乘等的一些基本知识等。除此之外的其他公式、我都尽量讲得更简单清楚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 线性判别分析(LDA,Linear Discriminant Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "LDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。有些资料上也称为是Fisher’s Linear Discriminant，因为它被Ronald Fisher发明自1936年.\n",
    "\n",
    "Discriminant这次词我个人的理解是，一个模型，不需要去通过概率的方法来训练、预测数据，比如说各种贝叶斯方法，就需要获取数据的先验、后验概率等等。LDA是在目前机器学习、数据挖掘领域经典且热门的一个算法，据我所知，百度的商务搜索部里面就用了不少这方面的算法。\n",
    "\n",
    "LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(Linear Classifier)：因为LDA是一种线性分类器。对于K-分类的一个分类问题，会有K个线性函数：\n",
    "\n",
    "$$y_k(x)=w_k^Tx+w_{k_0}$$\n",
    "\n",
    "当满足条件：对于所有的$j$，都有$Yk > Yj$,的时候，我们就说$x$属于类别$k$。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的，就是所属的分类了。\n",
    "\n",
    "上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：\n",
    "\n",
    "![](PCA0.gif)\n",
    "\n",
    "红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被原点明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：\n",
    "\n",
    "假设用来区分二分类的直线（投影函数)为：\n",
    "\n",
    "$$ y=w^T x $$\n",
    "\n",
    "LDA分类的一个目标是使得不同类别之间的距离越远越好，同一类别之中的距离越近越好，所以我们需要定义几个关键的值。\n",
    "\n",
    "类别i的原始中心点为：（Di表示属于类别i的点)\n",
    "\n",
    "$$ m_i=\\frac{1}{n_i}\\sum_{x\\in D_i}x $$\n",
    "\n",
    "类别i投影后的中心点为：\n",
    "\n",
    "$$ \\tilde{m_i}=w^T m_i $$\n",
    "\n",
    "衡量类别i投影后，类别点之间的分散程度（方差）为：\n",
    "\n",
    "$$ \\tilde{s_i}=\\sum_{y\\in Y_i}(y-\\tilde{m_i})^2 $$\n",
    "\n",
    "最终我们可以得到一个下面的公式，表示LDA投影到w后的损失函数：\n",
    "\n",
    "$$ J(w)=\\frac{|\\tilde{m_1}-\\tilde{m_2}|^2}{\\tilde{s_1}^2+\\tilde{s_2}^2} $$\n",
    "\n",
    "我们分类的目标是，`使得类别内的点距离越近越好`（集中），类别间的点越远越好。分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。\n",
    "\n",
    "我们定义一个投影前的各类别分散程度的矩阵，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的输入点集$D_i$里面的点距离这个分类的中心店$m_i$越近，则$S_i$里面元素的值就越小，如果分类的点都紧紧地围绕着$m_i$，则$S_i$里面的元素值越更接近0.\n",
    "\n",
    "$$ S_i=\\sum_{x\\in D_i}(x-m_i)(x-m_i)^T $$\n",
    "\n",
    "带入$S_i$，将$J(w)$分母化为：\n",
    "\n",
    "$$ \\tilde{s_i}=\\sum_{x\\in D_i}(w^T x-w^T m_i)^2=\\sum_{x\\in D_i}w^T(x-m_i)(x-m_i)^Tw=w^T S_i w $$\n",
    "\n",
    "$$ \\tilde{s_1}^2+\\tilde{s_2}^2=w^T(S_1+S_2)w=w^T S_w W $$\n",
    "\n",
    "同样的将J(w)分子化为：\n",
    "\n",
    "$$ |\\tilde{m_1}-\\tilde{m_2}|^2=w^T(m_1-m_2)(m_1-m_2)^T w=w^T S_B w $$\n",
    "\n",
    "这样损失函数可以化成下面的形式：\n",
    "\n",
    "$$ J(w)=\\frac{w^T S_B w}{w^T S_w w} $$\n",
    "\n",
    "   这样就可以用最喜欢的拉格朗日乘子法了，但是还有一个问题，如果分子、分母是都可以取任意值的，那就会使得有无穷解，我们将分母限制为长度为1（这是用拉格朗日乘子法一个很重要的技巧，在下面将说的PCA里面也会用到，如果忘记了，请复习一下高数），并作为拉格朗日乘子法的限制条件，带入得到：\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "c(w) &=w^T S_B w-\\lambda(w^T S_w w-1)\\\\\n",
    "\\Rightarrow \\frac{dc}{dw}&=2S_B w-2\\lambda S_w w=0\\\\\n",
    "\\Rightarrow S_B w & =\\lambda S_w w\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这样的式子就是一个求特征值的问题了。\n",
    "\n",
    "对于$N(N>2)$分类的问题，我就直接写出下面的结论了：\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "S_w & =\\sum_{i=1}^c S_i\\\\\n",
    "S_B & =\\sum_{i=1}^c n_i(m_i-m)(m_i-m)^T\\\\\n",
    "S_B w_i & = \\lambda S_w w_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的$W_i$了。\n",
    "\n",
    "这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。\n",
    "\n",
    "下图是图像识别中广泛用到的特征脸（eigen face），提取出特征脸有两个目的，首先是为了压缩数据，对于一张图片，只需要保存其最重要的部分就是了，然后是为了使得程序更容易处理，在提取主要特征的时候，很多的噪声都被过滤掉了。跟下面将谈到的PCA的作用非常相关。\n",
    "\n",
    "![](PCA5.png)\n",
    "\n",
    "特征值的求法有很多，求一个D * D的矩阵的时间复杂度是O(D^3), 也有一些求Top M的方法，比如说power method，它的时间复杂度是O(D^2 * M), 总体来说，求特征值是一个很费时间的操作，如果是单机环境下，是很局限的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 主成分分析(PCA,Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "主成分分析（PCA）与LDA有着非常近似的意思，LDA的输入数据是带标签的，而PCA的输入数据是不带标签的，所以PCA是一种unsupervised learning。\n",
    "\n",
    "LDA通常来说是作为一个独立的算法存在，给定了训练数据后，将会得到一系列的判别函数（discriminate function），之后对于新的输入，就可以进行预测了。\n",
    "\n",
    "而PCA更像是一个预处理的方法，它可以将原本的数据降低维度，而使得降低了维度的数据之间的方差最大（也可以说投影误差最小，具体在之后的推导里面会谈到）。\n",
    "\n",
    "方差这个东西是个很有趣的，有些时候我们会考虑减少方差（比如说训练模型的时候，我们会考虑到方差-偏差的均衡），有的时候我们会尽量的增大方差。方差就像是一种信仰（强哥的话），不一定会有很严密的证明，从实践来说，通过尽量增大投影方差的PCA算法，确实可以提高我们的算法质量。\n",
    "\n",
    "说了这么多，推推公式可以帮助我们理解。我下面将用两种思路来推导出一个同样的表达式。首先是最大化投影后的方差，其次是最小化投影后的损失（投影产生的损失最小）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 最大化方差法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "假设我们还是将一个空间中的点投影到一个向量中去。首先，给出原空间的中心点：\n",
    "\n",
    "$$ \\bar{x}=\\frac{1}{N}\\sum_{n=1}^{N}x_n $$\n",
    "\n",
    "假设u1为投影向量，投影之后的方差为：\n",
    "\n",
    "$$ \\frac{1}{N}\\sum_{n=1}^{N}\\{ u_i^T x_n-u_1^T \\bar{x} \\}^2 = u_1^T S u_1 $$\n",
    "\n",
    "上面这个式子如果看懂了之前推导LDA的过程，应该比较容易理解，如果线性代数里面的内容忘记了，可以再温习一下，优化上式等号右边的内容，还是用拉格朗日乘子法：\n",
    "\n",
    "$$ u_1^T S u_1+\\lambda_1(1-u_1^Tu_1) $$\n",
    "\n",
    "将上式求导，使之为0，得到：\n",
    "\n",
    "$$ Su_1-\\lambda_1 u_1 $$\n",
    "\n",
    "这是一个标准的特征值表达式了，λ对应的特征值，u对应的特征向量。上式的左边取得最大值的条件就是λ1最大，也就是取得最大的特征值的时候。假设我们是要将一个D维的数据空间投影到M维的数据空间中（M < D)， 那我们取前M个特征向量构成的投影矩阵就是能够使得方差最大的矩阵了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 最小化损失法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "假设输入数据x是在D维空间中的点，那么，我们可以用D个正交的D维向量去完全的表示这个空间（这个空间中所有的向量都可以用这D个向量的线性组合得到）。在D维空间中，有无穷多种可能找这D个正交的D维向量，哪个组合是最合适的呢？\n",
    "\n",
    "假设我们已经找到了这D个向量，可以得到：\n",
    "\n",
    "$$ x_n=\\sum_{i=1}^D \\alpha_{ni}u_i $$\n",
    "\n",
    "我们可以用近似法来表示投影后的点：\n",
    "\n",
    "$$ \\tilde{x_n}=\\sum_{i=1}^M z_{ni}u_i + \\sum_{i=M+1}^D b_iu_i $$\n",
    "\n",
    "上式表示，得到的新的x是由前$M$ 个基的线性组合加上后$D - M$个基的线性组合，注意这里的$z$是对于每个$x$都不同的，而b对于每个x是相同的，这样我们就可以用$M$个数来表示空间中的一个点，也就是使得数据降维了。但是这样降维后的数据，必然会产生一些扭曲，我们用$J$描述这种扭曲，我们的目标是，使得$J$最小：\n",
    "\n",
    "$$ J=\\frac{1}{N}\\sum_{n=1}^N \\| x_n-\\tilde{x_n} \\|^2 $$\n",
    "\n",
    "上式的意思很直观，就是对于每一个点，将降维后的点与原始的点之间的距离的平方和加起来，求平均值，我们就要使得这个平均值最小。我们令：\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial z_{nj}}=0 \\Rightarrow z_{nj}=x_n^T u_j, \\frac{\\partial J}{\\partial b_{j}}=0 \\Rightarrow b_{j}=\\bar{x}^T u_j,  $$\n",
    "\n",
    "将上面得到的$z$与$b$带入降维的表达式：\n",
    "\n",
    "$$ x_n-\\tilde{x_n}=\\sum_{i=M+1}^D \\{ (x_n-\\bar{x})u_i \\}u_i $$\n",
    "\n",
    "这里又是一个特征值的表达式，我们想要的前M个向量其实就是这里最大的M个特征值所对应的特征向量。证明这个还可以看看，我们$J$可以化为：\n",
    "\n",
    "$$ J=\\frac{1}{N}\\sum_{n=1}^N \\sum_{i=M+1}^D ( u_i^T x_n-u_1^T \\bar{x} )^2 = \\sum_{i=M+1}^D u_i^T S u_i $$\n",
    "\n",
    "也就是当误差$J$是由最小的$D - M$个特征值组成的时候，$J$取得最小值。跟上面的意思相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
