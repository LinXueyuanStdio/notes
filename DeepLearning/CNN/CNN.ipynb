{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从神经元到神经网络\n",
    "\n",
    "神经网络是多个“神经元”（感知机）的带权级联，神经网络算法可以提供非线性的复杂模型，它有两个参数：权值矩阵$W^l$和偏置向量$b^l$，不同于感知机的单一向量形式，$W^l$是复数个矩阵，$b^l$是复数个向量，其中的元素分别属于单个层，而每个层的组成单元，就是神经元。\n",
    "\n",
    "## 神经元\n",
    "\n",
    "神经网络是由多个“神经元”（感知机）组成的，每个神经元图示如下：\n",
    "\n",
    "![](cnn0.jpg)\n",
    "\n",
    "这其实就是一个单层感知机，其输入是由$x_1,x_2,x_3$和+1组成的向量，其输出为$h_{W,b}(x)=f(W^T x)=f(\\sum_{i=1}^{3}W_i x_i+b)$，其中$f$是一个激活函数，模拟的是生物神经元在接受一定的刺激之后产生兴奋信号，否则刺激不够的话，神经元保持抑制状态这种现象。这种由一个阈值决定两个极端的函数有点像示性函数，然而这里采用的是Sigmoid函数，其优点是连续可导。\n",
    "\n",
    "### Sigmoid函数\n",
    "\n",
    "常用的Sigmoid有两种——\n",
    "\n",
    "#### 单极性Sigmoid函数\n",
    "\n",
    "$$f(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "其图像如下\n",
    "\n",
    "![](cnn1.jpg)\n",
    "\n",
    "\n",
    "#### 双极性Sigmoid函数\n",
    "\n",
    "$$f(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$\n",
    "\n",
    "或者写成\n",
    "\n",
    "$$f(x)=\\frac{1-e^{-x}}{1+e^{-x}}$$\n",
    "\n",
    "把第一个式子分子分母同时除以$e^x$，令$x=-2x$就得到第二个式子了，换汤不换药。\n",
    "\n",
    "其图像如下\n",
    "\n",
    "![](cnn2.jpg)\n",
    "\n",
    "\n",
    "从它们两个的值域来看，两者名称里的极性应该指的是正负号。从导数来看，它们的导数都非常便于计算：\n",
    "\n",
    "对于$$f(x)=\\frac{1}{1+e^{-x}}$$有$f'(x)=f(x)(1-f(x))$，对于tanh，有$f'(x)=1-(f(x))^2$。\n",
    "\n",
    "> 1/(1+e^-x)求导的过程：\n",
    "  $$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx}\\sigma&=\\frac{d}{dx}(\\frac{1}{1+e^{-x}})\\\\\n",
    "&=\\frac{e^{-x}}{(1+e^{-x})^2}\\\\\n",
    "&=\\frac{(1+e^{-x})-1}{(1+e^{-x})^2}\\\\\n",
    "&=\\frac{(1+e^{-x})}{(1+e^{-x})^2}-\\frac{1}{(1+e^{-x})^2}\\\\\n",
    "&=\\sigma(x)-\\sigma(x)^2\\\\\n",
    "\\sigma' &= \\sigma(1-\\sigma)\n",
    "\\end{align}\n",
    "  $$\n",
    "  一旦知道了$f(x)$，就可以直接求$f'(x)$，所以说很方便。\n",
    "\n",
    "本Python实现使用的就是$\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    sigmoid 函数，1/(1+e^-x)\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0+math.exp(-x))\n",
    " \n",
    "def dsigmoid(y):\n",
    "    \"\"\"\n",
    "    sigmoid 函数的导数\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用双曲正切函数tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    sigmoid 函数，tanh \n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return math.tanh(x)\n",
    "\n",
    "def dsigmoid(y):\n",
    "    \"\"\"\n",
    "    sigmoid 函数的导数\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return 1.0 - y ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络模型(BPNN)\n",
    "\n",
    "神经网络就是多个神经元的级联，上一级神经元的输出是下一级神经元的输入，而且信号在两级的两个神经元之间传播的时候需要乘上这两个神经元对应的权值。例如，下图就是一个简单的神经网络：\n",
    "\n",
    "![](cnn3.jpg)\n",
    "\n",
    "其中，一共有一个输入层，一个隐藏层和一个输出层。输入层有3个输入节点，标注为+1的那个节点是偏置节点，偏置节点不接受输入，输出总是+1。\n",
    "\n",
    "定义上标为层的标号，下标为节点的标号，则本神经网络模型的参数是：$(W,b)=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$，其中$W_{ij}^{(l)}$是第$l$层的第$j$个节点与第$l+1$层第$i$个节点之间的连接参数（或称权值）；$b^{(l)}$表示第$l$层第$i$个偏置节点。这些符号在接下来的前向传播将要用到。\n",
    "\n",
    "### 前向传播\n",
    "\n",
    "如果后向传播对应训练的话，那么前向传播就对应预测（分类），并且训练的时候计算误差也要用到预测的输出值来计算误差。\n",
    "\n",
    "定义$a_i^{(l)}$为第$l$层第$i$个节点的激活值（输出值）。当$l=1$时，$a_i^{(1)}=x_i$。前向传播的目的就是在给定模型参数$W,b$的情况下，计算$l=2,3,4...$层的输出值，直到最后一层就得到最终的输出值。具体怎么算呢，以上图的神经网络模型为例：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_1^{(2)}&=f(W_{11}^{(1)}x_1+W_{12}^{(1)}x_2+W_{13}^{(1)}x_3+b_1^{(1)})\\\\\n",
    "a_2^{(2)}&=f(W_{21}^{(1)}x_1+W_{22}^{(1)}x_2+W_{23}^{(1)}x_3+b_2^{(1)})\\\\\n",
    "a_3^{(2)}&=f(W_{31}^{(1)}x_1+W_{32}^{(1)}x_2+W_{33}^{(1)}x_3+b_3^{(1)})\\\\\n",
    "h_{W,b}(x) &= f(a_{1}^{(3)}=f(W_{11}^{(2)}a_{1}^{(2)}+W_{12}^{(2)}a_2^{(2)}+W_{13}^{(2)}a_{3}^{(2)}+b_1^{(2)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这没什么稀奇的，核心思想是这一层的输出乘上相应的权值加上偏置量代入激活函数等于下一层的输入，一句大白话，所谓中文伪码。\n",
    "\n",
    "另外，追求好看的话可以把括号里面那个老长老长的加权和定义为一个参数：$z_{i}^{(l)}$表示第$l$层第$i$个节点的输入加权和，比如$z_{i}^{(2)}=\\sum_{j=1}^{n}W_{ij}^{(1)}x_j+b_{i}^{(1)}$。那么该节点的输出可以写作$a_{i}^{(l)}=f(z_{i}^{(l)})$。\n",
    "\n",
    "于是就得到一个好看的形式：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{(2)}&=W^{(1)}x+b^{(1)}\\\\\n",
    "a^{(2)}&=f(z^{(2)})\\\\\n",
    "z^{(3)}&=W^{(2)}a^{(2)}+b^{(2)}\\\\\n",
    "h_{W,b}(x)&=a^{(3)}=f(z^{(3)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "在这个好看的形式下，前向传播可以简明扼要地表示为：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{(l+1)}&=W^{(l)}a^{(l)}+b^{(l)}\\\\\n",
    "a^{(l+1)}&=f(z^{(l+1)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "在Python实现中，对应如下方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def runNN(self, inputs):\n",
    "        \"\"\"\n",
    "        前向传播进行分类\n",
    "        :param inputs:输入\n",
    "        :return:类别\n",
    "        \"\"\"\n",
    "        if len(inputs) != self.ni - 1:\n",
    "            print 'incorrect number of inputs'\n",
    " \n",
    "        for i in range(self.ni - 1):\n",
    "            self.ai[i] = inputs[i]\n",
    " \n",
    "        for j in range(self.nh):\n",
    "            sum = 0.0\n",
    "            for i in range(self.ni):\n",
    "                sum += ( self.ai[i] * self.wi[i][j] )\n",
    "            self.ah[j] = sigmoid(sum)\n",
    " \n",
    "        for k in range(self.no):\n",
    "            sum = 0.0\n",
    "            for j in range(self.nh):\n",
    "                sum += ( self.ah[j] * self.wo[j][k] )\n",
    "            self.ao[k] = sigmoid(sum)\n",
    " \n",
    "        return self.ao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，ai、ah、ao分别是输入层、隐藏层、输出层，而wi、wo则分别是输入层到隐藏层、隐藏层到输出层的权值矩阵。在本Python实现中，将偏置量一并放入了矩阵，这样进行线性代数运算就会方便一些。\n",
    "\n",
    "### 后向传播\n",
    "\n",
    "后向传播指的是在训练的时候，根据最终输出的误差来调整倒数第二层、倒数第三层……第一层的参数的过程。\n",
    "\n",
    "#### 符号定义\n",
    "\n",
    "$x_j^l$：第$l$层第$j$个节点的输入。\n",
    "\n",
    "$W_{ij}^l$：从第$l-1$层第$i$个节点到第$l$层第$j$个节点的权值。\n",
    "\n",
    "$\\sigma(x)=\\frac{1}{1+e^{-x}}$：Sigmoid函数。\n",
    "\n",
    "$\\theta_j^l$：第$l$层第$j$个节点的偏置。\n",
    "\n",
    "$O_j^l$：第$l$层第$j$个节点的输出。\n",
    "\n",
    "$t_j$：输出层第$j$个节点的目标值（Target value）。\n",
    "\n",
    "#### 输出层权值调整\n",
    "\n",
    "给定训练集$t_k$和模型输出$O_k$（这里没有上标l是因为这里在讨论输出层，l是固定的），输出层的输出误差（或称损失函数吧）定义为：\n",
    "\n",
    "$$E=\\frac{1}{2}\\sum_{k\\in K}(O_k-t_k)^2$$\n",
    "\n",
    "其实就是所有实例对应的误差的平方和的一半，训练的目标就是最小化该误差。怎么最小化呢？看损失函数对参数的导数$\\frac{\\partial E}{\\partial W_{jk}^{l}}$呗。\n",
    "\n",
    "将$E$的定义代入该导数：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}}=\\frac{\\partial}{\\partial W_{jk}}\\frac{1}{2}\\sum_{k\\in K}(O_k-t_k)^2$$\n",
    "\n",
    "无关变量拿出来：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}}=(O_k-t_k)\\frac{\\partial}{\\partial W_{jk}}O_k$$\n",
    "\n",
    "看到这里大概明白为什么非要把误差定义为误差平方和的一半了吧，就是为了好看，数学家都是外貌协会的。\n",
    "\n",
    "将$O_k=\\sigma(x_k)$（输出层的输出等于输入代入Sigmoid函数）这个关系代入有：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}}=(O_k-t_k)\\frac{\\partial}{\\partial W_{jk}}\\sigma(x_k)$$\n",
    "\n",
    "对Sigmoid求导有：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}}=(O_k-t_k)\\sigma(x_k)(1-\\sigma(x_k))\\frac{\\partial}{\\partial W_{jk}}x_k$$\n",
    "\n",
    "要开始耍小把戏了，由于输出层第$k$个节点的输入$x_k$等于上一层第$j$个节点的输出$O_j$乘上$W_{jk}$，即$x_k=O_kW_{jk}$，而上一层的输出$O_j$是与到输出层的权值变量$W_{jk}$无关的，可以看做一个常量，是线性关系。所以对$x_k$求权值变量$W_{jk}$的偏导数直接等于$O_j$，也就是说：$\\frac{\\partial}{\\partial W_{jk}}x_k=\\frac{\\partial}{\\partial W_{jk}}(O_j W_{jk})=O_j$。\n",
    "\n",
    "然后将上面用过的$\\sigma(x_k)=O_k$代进去就得到最终的：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}}=(O_k-t_k)O_k(1-O_k)O_j$$\n",
    "\n",
    "为了表述方便将上式记作：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{jk}}=O_k\\delta_k$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\\delta=(O_k-t_k)O_k(1-O_k)O_j$$\n",
    "\n",
    "\n",
    "\n",
    "#### 隐藏层权值调整\n",
    "\n",
    "依然采用类似的方法求导，只不过求的是关于隐藏层和前一层的权值参数的偏导数：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=\\frac{\\partial}{\\partial W_{ij}}\\frac{1}{2}\\sum_{k\\in K}(O_k-t_k)^2$$\n",
    "\n",
    "\n",
    "老样子：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=\\sum_{k\\in K}(O_k-t_k)\\frac{\\partial}{\\partial W_{ij}}O_k$$\n",
    "\n",
    "\n",
    "还是老样子：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=\\sum_{k\\in K}(O_k-t_k)\\frac{\\partial}{\\partial W_{ij}}\\sigma(x_k)$$\n",
    "\n",
    "还是把Sigmoid弄进去：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=\\sum_{k\\in K}(O_k-t_k)\\sigma(x_k)(1-\\sigma(x_k))\\frac{\\partial x_k}{\\partial W_{ij}}$$\n",
    "\n",
    "把$\\sigma(x_k)=O_k$代进去，并且将导数部分拆开：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=\\sum_{k\\in K}(O_k-t_k)O_k(1-O_k)\\frac{\\partial x_k}{\\partial O_{j}}\\cdot\\frac{\\partial O_j}{\\partial W_{ij}}$$\n",
    "\n",
    "\n",
    "又要耍把戏了，输出层的输入等于上一层的输出乘以相应的权值，亦即$x_k=W_{jk}O_j$，于是得到：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=\\sum_{k\\in K}(O_k-t_k)O_k(1-O_k)W_{jk}\\frac{\\partial O_j}{\\partial W_{ij}}$$\n",
    "\n",
    "把最后面的导数挪到前面去，接下来要对它动刀了：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=\\frac{\\partial O_j}{\\partial W_{ij}}\\sum_{k\\in K}(O_k-t_k)O_k(1-O_k)W_{jk}$$\n",
    "\n",
    "再次利用$O_k=\\sigma(x_k)$，这对$j$也成立，代进去：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=O_j(1-O_j)\\frac{\\partial x_j}{\\partial W_{ij}}\\sum_{k\\in K}(O_k-t_k)O_k(1-O_k)W_{jk}$$\n",
    "\n",
    "再次利用$x_k=W_{jk}O_j$，$j$换成$i$,$k$换成$j$也成立，代进去：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=O_j(1-O_j)O_i\\sum_{k\\in K}(O_k-t_k)O_k(1-O_k)W_{jk}$$\n",
    "\n",
    "利用刚才定义的$\\delta_k$，最终得到：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=O_iO_j(1-O_j)\\sum_{k\\in K}\\delta_k W_{jk}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\\delta=O_k(1-O_k)(O_k-t_k)$$\n",
    "\n",
    "我们还可以仿照$\\delta_k$的定义来定义一个$\\delta_j$，得到：\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{ij}}=O_i\\delta_j$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\\delta_j=O_j(1-O_j)\\sum_{k\\in K}\\delta_k W_{jk}$$\n",
    "\n",
    "#### 偏置的调整\n",
    "\n",
    "因为没有任何节点的输出流向偏置节点，所以偏置节点不存在上层节点到它所对应的权值参数，也就是说不存在关于权值变量的偏导数。虽然没有流入，但是偏置节点依然有输出（总是+1），该输出到下一层某个节点的时候还是会有权值的，对这个权值依然需要更新。\n",
    "\n",
    "我们可以直接对偏置求导，发现：\n",
    "\n",
    "$$\\frac{\\partial O}{\\partial \\theta}=O(1-O)\\frac{\\partial \\theta}{\\partial \\theta}$$\n",
    "\n",
    "原视频中说$\\frac{\\partial O}{\\partial \\theta}=1$，这是不对的，作者也在讲义中修正了这个错误，$\\frac{\\partial O}{\\partial \\theta}=O(1–O)$。\n",
    "\n",
    "然后再求$\\frac{\\partial E}{\\partial \\theta}$，$\\frac{\\partial E}{\\partial \\theta}=\\sum_{k \\in K}(O_k-t_k)\\frac{\\partial}{\\partial \\theta}O_k$，后面的导数等于$\\frac{\\partial O}{\\partial \\theta}=O(1-O)$，代进去有\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\theta}=\\delta_l$$\n",
    "\n",
    "其中，\n",
    "\n",
    "$$\\delta_k=O_k(1-O_k)(O_k-t_k)$$\n",
    "\n",
    "#### 后向传播算法步骤\n",
    "\n",
    "- 随机初始化参数，对输入利用前向传播计算输出。\n",
    "\n",
    "- 对每个输出节点按照下式计算$\\delta$：$\\delta_k=O_k(1-O_k)(O_k-t_k)$\n",
    "\n",
    "- 对每个隐藏节点按照下式计算$\\delta$：$\\delta_j=O_j(1-O_j)\\sum_{k\\in K}\\delta_k W_{jk}$\n",
    "\n",
    "- 计算梯度$\\Delta W=-\\eta\\delta_lO_{l-1},\\Delta \\theta=-\\eta\\delta_l$，并更新权值参数和偏置参数：$W+\\Delta W\\to W,\\theta+\\Delta \\theta \\to \\theta$。这里的$\\eta$是学习率，影响训练速度。\n",
    "\n",
    "#### 后向传播算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def backPropagate(self, targets, N, M):\n",
    "        \"\"\"\n",
    "        后向传播算法\n",
    "        :param targets: 实例的类别 \n",
    "        :param N: 本次学习率\n",
    "        :param M: 上次学习率\n",
    "        :return: 最终的误差平方和的一半\n",
    "        \"\"\"\n",
    "        # http://www.youtube.com/watch?v=aVId8KMsdUU&feature=BFa&list=LLldMCkmXl4j9_v0HeKdNcRA\n",
    " \n",
    "        # 计算输出层 deltas\n",
    "        # dE/dw[j][k] = (t[k] - ao[k]) * s'( SUM( w[j][k]*ah[j] ) ) * ah[j]\n",
    "        output_deltas = [0.0] * self.no\n",
    "        for k in range(self.no):\n",
    "            error = targets[k] - self.ao[k]\n",
    "            output_deltas[k] = error * dsigmoid(self.ao[k])\n",
    " \n",
    "        # 更新输出层权值\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                # output_deltas[k] * self.ah[j] 才是 dError/dweight[j][k]\n",
    "                change = output_deltas[k] * self.ah[j]\n",
    "                self.wo[j][k] += N * change + M * self.co[j][k]\n",
    "                self.co[j][k] = change\n",
    " \n",
    "        # 计算隐藏层 deltas\n",
    "        hidden_deltas = [0.0] * self.nh\n",
    "        for j in range(self.nh):\n",
    "            error = 0.0\n",
    "            for k in range(self.no):\n",
    "                error += output_deltas[k] * self.wo[j][k]\n",
    "            hidden_deltas[j] = error * dsigmoid(self.ah[j])\n",
    " \n",
    "        # 更新输入层权值\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                change = hidden_deltas[j] * self.ai[i]\n",
    "                # print 'activation',self.ai[i],'synapse',i,j,'change',change\n",
    "                self.wi[i][j] += N * change + M * self.ci[i][j]\n",
    "                self.ci[i][j] = change\n",
    " \n",
    "        # 计算误差平方和\n",
    "        # 1/2 是为了好看，**2 是平方\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error = 0.5 * (targets[k] - self.ao[k]) ** 2\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意不同于上文的单一学习率$\\eta$，这里有两个学习率$N$和$M$。$N$相当于上文的$\\eta$，而$M$则是在用上次训练的梯度更新权值时的学习率。这种同时考虑最近两次迭代得到的梯度的方法，可以看做是对单一学习率的改进。\n",
    "\n",
    "> 这里并没有出现任何更新偏置的操作，为什么？\n",
    "\n",
    "> 因为这里的偏置是单独作为一个偏置节点放到输入层里的，它的值（输出，没有输入）固定为1，它的权值已经自动包含在上述权值调整中了。\n",
    "  如果将偏置作为分别绑定到所有神经元的许多值，那么则需要进行偏置调整，而不需要权值调整（此时没有偏置节点）。\n",
    "  哪个方便，当然是前者了，这也导致了大部分神经网络实现都采用前一种做法。\n",
    "\n",
    "### 完整的实现\n",
    "\n",
    "#### 对象式实现\n",
    "\n",
    "[完整的实现]()\n",
    "\n",
    "直接运行bpnn.py即可得到输出：\n",
    "\n",
    "```shell\n",
    "Combined error 0.171204877501\n",
    "Combined error 0.190866985872\n",
    "Combined error 0.126126875154\n",
    "Combined error 0.0658488960415\n",
    "Combined error 0.0353249077599\n",
    "Combined error 0.0214428399072\n",
    "Combined error 0.0144886807614\n",
    "Combined error 0.0105787745309\n",
    "Combined error 0.00816264126944\n",
    "Combined error 0.00655731212209\n",
    "Combined error 0.00542964723539\n",
    "Combined error 0.00460235328667\n",
    "Combined error 0.00397407912435\n",
    "Combined error 0.00348339081276\n",
    "Combined error 0.00309120476889\n",
    "Combined error 0.00277163178862\n",
    "Combined error 0.00250692771135\n",
    "Combined error 0.00228457151714\n",
    "Combined error 0.00209550313514\n",
    "Combined error 0.00193302192499\n",
    "Inputs: [0, 0] --> [0.9982333356008245] \tTarget [1]\n",
    "Inputs: [0, 1] --> [0.9647325217906978] \tTarget [1]\n",
    "Inputs: [1, 0] --> [0.9627966274767186] \tTarget [1]\n",
    "Inputs: [1, 1] --> [0.05966109502803293] \tTarget [0]\n",
    "```\n",
    "IBM利用Neil Schemenauer的这一模块（旧版）做了一个识别代码语言的例子，我将其更新到新版，已经整合到了项目中。\n",
    "\n",
    "要运行测试的话，执行命令\n",
    "\n",
    "```shell\n",
    "code_recognizer.py testdata.200\n",
    "```\n",
    "\n",
    "即可得到输出：\n",
    "\n",
    "```shell\n",
    "ERROR_CUTOFF = 0.01\n",
    "INPUTS = 20\n",
    "ITERATIONS = 1000\n",
    "MOMENTUM = 0.1\n",
    "TESTSIZE = 500\n",
    "OUTPUTS = 3\n",
    "TRAINSIZE = 500\n",
    "LEARNRATE = 0.5\n",
    "HIDDEN = 8\n",
    "Targets: [1, 0, 0] -- Errors: (0.000 OK)   (0.001 OK)   (0.000 OK)   -- SUCCESS!\n",
    "```\n",
    "\n",
    "值得一提的是，这里的HIDDEN = 8指的是隐藏层的节点个数，不是层数，层数多了就变成DeepLearning了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数式实现\n",
    "\n",
    "接下来秀下操作，比如，只用9行\n",
    "\n",
    "尽管我们不直接用神经网络库，但还是要从Python数学库Numpy中导入4种方法：\n",
    "\n",
    "- exp： 自然对常数\n",
    "- array： 创建矩阵\n",
    "- dot：矩阵乘法\n",
    "- random： 随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurency: [ 0.99993704]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "random.seed(1)\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "for iteration in range(10000):\n",
    "    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))\n",
    "print(\"accurency:\",1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机的初始突触权重：\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "训练后的突触权重：\n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "考虑新的形势 [1, 0, 0] -> ?: \n",
      "[ 0.99993704]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # 随机数发生器种子，以保证每次获得相同结果\n",
    "        random.seed(1)\n",
    "\n",
    "        # 对单个神经元建模，含有3个输入连接和一个输出连接\n",
    "        # 对一个3 x 1的矩阵赋予随机权重值。范围-1～1，平均值为0\n",
    "        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "    # Sigmoid函数，S形曲线\n",
    "    # 用这个函数对输入的加权总和做正规化，使其范围在0～1\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # Sigmoid函数的导数\n",
    "    # Sigmoid曲线的梯度\n",
    "    # 表示我们对当前权重的置信程度\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # 通过试错过程训练神经网络\n",
    "    # 每次都调整突触权重\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # 将训练集导入神经网络\n",
    "            output = self.think(training_set_inputs)\n",
    "\n",
    "            # 计算误差（实际值与期望值之差）\n",
    "            error = training_set_outputs - output\n",
    "\n",
    "            # 将误差、输入和S曲线梯度相乘\n",
    "            # 对于置信程度低的权重，调整程度也大\n",
    "            # 为0的输入值不会影响权重\n",
    "            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "\n",
    "            # 调整权重\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # 神经网络一思考\n",
    "    def think(self, inputs):\n",
    "        # 把输入传递给神经网络\n",
    "        return self.__sigmoid(dot(inputs, self.synaptic_weights))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 初始化神经网络\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"随机的初始突触权重：\")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # 训练集。四个样本，每个有3个输入和1个输出\n",
    "    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "    # 用训练集训练神经网络\n",
    "    # 重复一万次，每次做微小的调整\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "    print(\"训练后的突触权重：\")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # 用新数据测试神经网络\n",
    "    print(\"考虑新的形势 [1, 0, 0] -> ?: \")\n",
    "    print(neural_network.think(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从BPNN到DNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1) 随机初始突触权重： \n",
      "    Layer 1 (4 neurons, each with 3 inputs): \n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "    Layer 2 (1 neuron, with 4 inputs):\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n",
      "Stage 2) 训练后的新权重值： \n",
      "    Layer 1 (4 neurons, each with 3 inputs): \n",
      "[[ 0.3122465   4.57704063 -6.15329916 -8.75834924]\n",
      " [ 0.19676933 -8.74975548 -6.1638187   4.40720501]\n",
      " [-0.03327074 -0.58272995  0.08319184 -0.39787635]]\n",
      "    Layer 2 (1 neuron, with 4 inputs):\n",
      "[[ -8.18850925]\n",
      " [ 10.13210706]\n",
      " [-21.33532796]\n",
      " [  9.90935111]]\n",
      "Stage 3) 思考新形势 [1, 1, 0] -> ?: \n",
      "[ 0.0078876]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "\n",
    "class NeuronLayer():\n",
    "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n",
    "        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layer1, layer2):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "\n",
    "    # Sigmoid函数，S形曲线\n",
    "    # 传递输入的加权和，正规化为0-1\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # Sigmoid函数的导数，Sigmoid曲线的梯度，表示对现有权重的置信程度\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # 通过试错训练神经网络，每次微调突触权重\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # 将整个训练集传递给神经网络\n",
    "            output_from_layer_1, output_from_layer_2 = self.think(training_set_inputs)\n",
    "\n",
    "            # 计算第二层的误差\n",
    "            layer2_error = training_set_outputs - output_from_layer_2\n",
    "            layer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)\n",
    "\n",
    "            # 计算第一层的误差，得到第一层对第二层的影响\n",
    "            layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)\n",
    "            layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\n",
    "\n",
    "            # 计算权重调整量\n",
    "            layer1_adjustment = training_set_inputs.T.dot(layer1_delta)\n",
    "            layer2_adjustment = output_from_layer_1.T.dot(layer2_delta)\n",
    "\n",
    "            # 调整权重\n",
    "            self.layer1.synaptic_weights += layer1_adjustment\n",
    "            self.layer2.synaptic_weights += layer2_adjustment\n",
    "\n",
    "    # 神经网络一思考\n",
    "    def think(self, inputs):\n",
    "        output_from_layer1 = self.__sigmoid(dot(inputs, self.layer1.synaptic_weights))\n",
    "        output_from_layer2 = self.__sigmoid(dot(output_from_layer1, self.layer2.synaptic_weights))\n",
    "        return output_from_layer1, output_from_layer2\n",
    "\n",
    "    # 输出权重\n",
    "    def print_weights(self):\n",
    "        print(\"    Layer 1 (4 neurons, each with 3 inputs): \")\n",
    "        print(self.layer1.synaptic_weights)\n",
    "        print(\"    Layer 2 (1 neuron, with 4 inputs):\")\n",
    "        print(self.layer2.synaptic_weights)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 设定随机数种子\n",
    "    random.seed(1)\n",
    "\n",
    "    # 创建第一层 (4神经元, 每个3输入)\n",
    "    layer1 = NeuronLayer(4, 3)\n",
    "\n",
    "    # 创建第二层 (单神经元，4输入)\n",
    "    layer2 = NeuronLayer(1, 4)\n",
    "\n",
    "    # 组合成神经网络\n",
    "    neural_network = NeuralNetwork(layer1, layer2)\n",
    "\n",
    "    print(\"Stage 1) 随机初始突触权重： \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    # 训练集，7个样本，均有3输入1输出\n",
    "    training_set_inputs = array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "    training_set_outputs = array([[0, 1, 1, 1, 1, 0, 0]]).T\n",
    "\n",
    "    # 用训练集训练神经网络\n",
    "    # 迭代60000次，每次微调权重值\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 60000)\n",
    "\n",
    "    print(\"Stage 2) 训练后的新权重值： \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    # 用新数据测试神经网络\n",
    "    print(\"Stage 3) 思考新形势 [1, 1, 0] -> ?: \")\n",
    "    hidden_state, output = neural_network.think(array([1, 1, 0]))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
